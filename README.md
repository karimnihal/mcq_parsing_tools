# MCQ Parsing Tools

This repository contains a set of Python tools designed for parsing and evaluating multiple-choice question (MCQ) answers, particularly when dealing with outputs from language models (LLMs). The tools are useful for processing potentially noisy text data to extract structured MCQ answers (e.g., 'a', 'b', 'c', 'd', 'x') and comparing different parsing approaches, with consideration for multilingual datasets.

## Overview

Extracting reliable MCQ answers from free-form text, especially LLM generations which can include reasoning, refusals, or inconsistent formatting, is a common challenge. This toolkit provides:

1.  An **automated parser** (`mcq_parser`) using regex and optional LLM assistance (via Ollama) for efficient, large-scale processing.
2.  An **interactive manual parser** (`manual_parser`) for creating or correcting ground-truth labels row by row.
3.  A **comparison script** (`compare_parser.py`) to evaluate the automated parser's output against the manual one.

These tools facilitate the creation of reliable MCQ datasets and the evaluation of automated extraction methods.

## Folder Structure

```
MCQ_PARSING_TOOLS/
├── manual_parser/             # Interactive manual parsing tool
│   ├── manual_parser.py
│   ├── README.md              # Detailed instructions for manual_parser
│   └── test_manual_parser.py
├── mcq_parser/                # Automated parsing tool (Regex + optional LLM)
│   ├── example_few_shot.txt   # Example file for LLM few-shot prompting
│   ├── mcq_parser.py
│   ├── README.md              # Detailed instructions for mcq_parser
│   └── test_mcq_parser.py
├── compare_parser.py          # Script to compare outputs (e.g., manual vs. auto)
├── README.md                  # This main project README
├── sample.tsv                 # Sample input data file
├── sample_manual_parsed.tsv   # Sample output from manual_parser on sample.tsv
└── .gitignore
```

## Components

### 1. Automated Parser (`mcq_parser/`)

This tool automatically extracts MCQ answers from a specified column in an input file.

*   **Functionality:** It primarily uses robust regular expressions to find patterns indicating an MCQ choice (e.g., "Answer: A", "(b)", "Option 3").
*   **LLM Integration (Optional):** It can optionally use a local LLM via Ollama (like Llama 3, Mistral, etc.) as a fallback for rows where regex fails, or as the *primary* method (skipping regex). This can improve accuracy on complex or poorly formatted text.
*   **Features:** Supports few-shot prompting for the LLM, Chain of Thought (CoT) reasoning, various option formats (a-d, 1-6, etc.), configurable handling of non-answers ('x'), and calculation of accuracy metrics if reference answers are provided.
*   **Details:** See the [mcq_parser README](./mcq_parser/README.md) for full usage instructions, arguments, and Ollama setup.

### 2. Manual Review & Correction Tool (`manual_parser/`)

This tool provides a command-line interface for human-in-the-loop parsing or verification.

*   **Functionality:** It steps through rows of an input file, displays text from a source column, shows any existing parsed answer, and prompts the user to enter or confirm the correct MCQ answer (e.g., 'a', 'b', 'c', 'd', 'x').
*   **Use Cases:** Ideal for creating gold-standard datasets, correcting errors from automated parsers, or handling ambiguous cases that automated tools struggle with.
*   **Features:** Supports resuming interrupted sessions, flexible input/output formats (CSV, TSV), configurable answer options, and can align data using a unique index column or process sequentially.
*   **Details:** See the [manual_parser README](./manual_parser/README.md) for full usage instructions and arguments.

### 3. Comparison Script (`compare_parser.py`)

This utility script compares the parsed answers from two different files, typically the output of the automated `mcq_parser` against the human-verified output of `manual_parser`.

*   **Functionality:** It reads two input files, matches rows based on a shared unique index column, compares the values in the designated parsed answer columns, and reports agreement/disagreement counts and percentages.
*   **Use Case:** Useful for evaluating the performance (accuracy/consistency) of the automated `mcq_parser` against a manual "gold standard".
*   **Usage:**
    *   The script currently requires **editing the file itself** to configure the input paths and column names.
    *   Open `compare_parser.py` and modify the following variables near the top:
        *   `file1_path`: Path to the first file (e.g., the manual parsed file like `sample_manual_parsed.tsv`).
        *   `file2_path`: Path to the second file (e.g., the output file generated by `mcq_parser.py`).
        *   `index_col`: The name of the column containing unique IDs present in *both* files (e.g., `"Unnamed: 0"`, `"question_id"`).
        *   `column1_name`: The name of the parsed answer column in the first file (e.g., `"manual_label"`).
        *   `column2_name`: The name of the parsed answer column in the second file (e.g., the `--target` column name used in `mcq_parser.py` which gets overwritten with the final label).
    *   Run the script from the command line:
        ```bash
        python compare_parser.py
        ```
    *   It will print the comparison results to the console.

## Workflow & Sample Files

A typical workflow might be:

1.  Start with raw data containing LLM outputs (like `sample.tsv`).
2.  **Option A (Create Gold Standard):** Use `manual_parser.py` on `sample.tsv` to interactively create a file with verified answers (like `sample_manual_parsed.tsv`, which includes the original data plus a `manual_label` column added by the tool).
    ```bash
    # Example command that could generate sample_manual_parsed.tsv
    python manual_parser/manual_parser.py \
        --input sample.tsv \
        --output sample_manual_parsed.tsv \
        --source-col model_output_text \
        --parsed-col manual_label \
        --index-col "Unnamed: 0"
    ```
3.  **Option B (Automated Parsing):** Use `mcq_parser.py` on `sample.tsv` to automatically extract answers into a new file (e.g., `results_auto.tsv`). You might provide the `manual_label` from step 2 as the `--reference` for automatic accuracy calculation.
    ```bash
    # Example command for automated parsing
    python mcq_parser/mcq_parser.py \
        --input sample.tsv \
        --output results_metrics.csv \
        --target model_output_text \
        # Optionally provide reference for accuracy calc: --reference manual_label (if sample.tsv had it)
        # Add --use-llm, --skip-regex etc. as needed
        # Note: Processed data is saved to results_auto_processed_data.tsv (derived from --output)
    ```
4.  **Compare:** Edit and run `compare_parser.py` to compare the manually parsed answers (`sample_manual_parsed.tsv`, column `manual_label`) against the automatically parsed ones (e.g., `results_auto_processed_data.tsv`, column `model_output_text`) using their shared index.

*   `sample.tsv`: Provides example input data with an index (`Unnamed: 0`) and a text column (`model_output_text`) to parse.
*   `sample_manual_parsed.tsv`: Shows the result of running `manual_parser` on `sample.tsv`. It contains all original columns plus the human-added `manual_label` column.

## Installation

Requires Python 3. Key dependencies include:

*   `pandas` (used by all scripts)
*   `scikit-learn` (used by `mcq_parser` for metrics)
*   `requests` (used by `mcq_parser` for Ollama API calls)

Install common dependencies:

```bash
pip install pandas scikit-learn requests
```

Refer to the README files within the `mcq_parser` and `manual_parser` directories for any specific setup, especially regarding Ollama installation and model pulling if you intend to use the LLM features of `mcq_parser`.

---